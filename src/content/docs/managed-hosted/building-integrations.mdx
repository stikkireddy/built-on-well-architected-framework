---
title: Building Integrations
description: Attribution (Telemetry) methodology for Managed Hosted deployments
---

# Building Integrations

## Attribution (Telemetry) Methodology

### Introduction

Telemetry plays a critical role in assessing partnership performance within Databricks Technology Partner integrations. It allows Databricks and its partners to monitor integration usage, attribute activity to specific products, and gain insights into joint customer attribution.

This document outlines Databricks approach to partner integration telemetry, explaining how User-Agent identifiers enable attribution/telemetry, their significance, the recommended formatting standards, and guidelines on how partners must implement them across supported integration types.

### What Is a User-Agent?

A User-Agent in Databricks Technology (ISV) partner integrations is a unique identifier associated with the partner product when integrating with Databricks. This identifier supports connections established through Lakehouse APIs, SDKs, SQL connectors, drivers, and Lakebase integrations.

For library-based integrations, the attribution is based on the library or class name, rather than a runtime connection parameter.

### Why Is a User-Agent Required?

Databricks requires User-Agent tagging in partner integrations for several reasons:

- **Telemetry and Usage Attribution**: Enables Databricks to track partner product usage and gather metrics for analysis and reporting.
- **Joint Customer Attribution**: Provides visibility into how (many) joint customers interact with partner integrations.
- **Go-To-Market (GTM) Initiatives**: Support the measurement of integration adoption and the effectiveness of collaborative partner activities.

### User-Agent Format (Recommended)

Partners must use the following format:

```
<isv-name+product-name>/<product-version>
```

#### Guidelines:

- **isv-name (Required)**: Partner company name. The ISV name must be standard across all Partner products.
- **product-name (Required)**: The product used in this integration.
- **product-version (Optional)**: The version of the product used in this integration. Including this allows better traceability for you and Databricks.
  - **Note**: It is up to the Partner team to keep the version number up to date

Partners must consistently implement the User-Agent across all implemented integration types and share the corresponding value(s) with the Databricks Partner Engineering team for tracking telemetry.

Each product or integration must have a distinct User-Agent value to ensure reliable and granular attribution.

**Note**: The partner product must set the User-Agent value internally in the Databricks code path connection logic (e.g., JDBC, ODBC, SDK) rather than relying on the joint customers to configure it.

### Supported Integration Mechanisms

Several Databricks integration types support implementing the User-Agent tag. The table(s) below outline configuration steps for each integration type, with links to detailed setup guidance.

#### Overview: Lakehouse Integrations

| Integration Type | Components | Attribution Methodology |
|-----------------|------------|------------------------|
| SQL Drivers | JDBC, ODBC, Node.js, Go | Captures the User-Agent string directly from the driver connection parameters in Databricks telemetry. |
| SQL Connectors and Frameworks | Python SQL Connector, SQLAlchemy, PyODBC | Captures UserAgentEntry or _user_agent_entry from the connector session metadata during SQL connection initialization. |
| SDKs | Databricks SDK for Python, Go, Java | SDKs automatically embed the User-Agent identifiers into every API call header sent from the SDK client. |
| REST APIs | Unity Catalog REST APIs, Iceberg Integration | Databricks telemetry extracts User-Agent from HTTP request headers on all API endpoints. |
| Databricks Connect | Scala, Python | |
| Libraries / UDFs | Python or Scala Libraries | Attribution is derived from the top-level Python/Scala class name. |

#### Overview: Lakebase Integrations

| Integration Type | Components | Attribution Methodology |
|-----------------|------------|------------------------|
| Lakebase Integrations | JDBC, psql Client, psycopg2/3, SQLAlchemy | Captures the application_name field during the initial PostgreSQL-compatible connection handshake in Databricks Lakebase. |

## Lakehouse Integrations

### SQL Drivers

#### Databricks JDBC Driver

Partner products must set the UserAgentEntry parameter when connecting to Databricks through the JDBC driver.

##### 1. Setting UserAgentEntry using JDBC URL (DriverManager)

Add UserAgentEntry directly to the JDBC connection string:

```java
String jdbcUrl =
    "jdbc:databricks://<host>:443/default;" +
    "AuthMech=11;" +
    "Auth_Flow=1;" +
    "OAuth2ClientId=<client-id>;" +
    "OAuth2Secret=<client-secret>;" +
    "HttpPath=<http-path>;" +
    "SSL=1;" +
    "UserAgentEntry=<isv-name+product-name>/<product-version>;";

Connection conn = DriverManager.getConnection(jdbcUrl);
```

##### 2. Setting UserAgentEntry using DataSource

The UserAgentEntry parameter is added as part of the connection properties when using a JDBC DataSource.

```java
import com.databricks.client.jdbc.DataSource;
....
DataSource ds = new DataSource();

Properties props = new Properties();
props.setProperty("UserAgentEntry", "<isv-name+product-name>/<product-version>");

// Add required connection properties
props.setProperty("AuthMech", "11");
props.setProperty("Auth_Flow", "1");
props.setProperty("OAuth2ClientId", "<client-id>");
props.setProperty("OAuth2Secret", "<client-secret>");
props.setProperty("SSL", "1");

// Apply the properties to the DataSource
ds.setProperties(props);

Connection conn = ds.getConnection();
```

#### Databricks ODBC Driver

For the Databricks (Simba) ODBC driver, add UserAgentEntry to the DSN or connection string:

**Example – DSN (non-Windows):**

```ini
[Databricks]
Driver=<path-to-driver>
Host=<server-hostname>
Port=443
HTTPPath=<http-path>
SSL=1
ThriftTransport=2
UserAgentEntry=<isv-name+product-name>/<product-version>
```

**Example – DSN-less connection string:**

```
Driver=<path-to-driver>;
Host=<server-hostname>;
Port=443;
HTTPPath=<http-path>;
SSL=1;
ThriftTransport=2;
UserAgentEntry=isv-name+product-name/<product-version>;
```

#### Databricks Node.js Driver

Partners must specify the User-Agent identifier using the userAgentEntry property when creating the client connection:

```javascript
const { DBSQLClient } = require('@databricks/sql');

const client = new DBSQLClient();
client.connect({
  host: process.env.DATABRICKS_SERVER_HOSTNAME,
  path: process.env.DATABRICKS_HTTP_PATH,
  ......
  userAgentEntry: '<isv-name+product-name>/<product-version>',
});
```

#### Databricks Go SQL Driver

Partners must include the User-Agent identifier using the WithUserAgentEntry option when creating the connector:

```go
connector, err := dbsql.NewConnector(
  .....
  dbsql.WithServerHostname(os.Getenv("DATABRICKS_HOST")),
  dbsql.WithPort(443),
  dbsql.WithHTTPPath(os.Getenv("DATABRICKS_HTTP_PATH")),
  dbsql.WithUserAgentEntry("<isv-name+product-name>/<product-version>"),
)
```

### SQL Connectors and Frameworks

#### Python SQL Connector

Partners must include the User-Agent identifier using the `_user_agent_entry` parameter when creating the connection (note the leading underscore):

```python
from databricks import sql
import os

with sql.connect(
    server_hostname=os.getenv("DATABRICKS_HOST"),
    http_path=os.getenv("DATABRICKS_HTTP_PATH"),
    credentials_provider=credential_provider,
    _user_agent_entry="<isv-name+product-name>/<product-version>"
) as connection:
```

#### SQLAlchemy

Partners must set the User-Agent identifier through the user_agent_entry parameter in the create_engine() configuration under the connect_args:

```python
from sqlalchemy import create_engine

engine = create_engine(
    "databricks+pyodbc://token:<ACCESS_TOKEN>@<SERVER_HOST>:443",
    connect_args={
        "http_path": "<HTTP_PATH>",
        "user_agent_entry": "<isv-name+product-name>/<product-version>"
    }
)
```

#### PyODBC (Databricks ODBC Driver)

Partners must include the UserAgentEntry parameter in their connection setup.

**Example 1 – DSN connection**

```python
import pyodbc

conn = pyodbc.connect(
    "DSN=<dsn-name>;UserAgentEntry=<isv-name+product-name>/<product-version>",
    autocommit=True
)
cursor = conn.cursor()
cursor.execute("SELECT * FROM samples.nyctaxi.trips")

for row in cursor.fetchall():
    print(row)
```

**Example 2 – DSN-less connection**

```python
import pyodbc
import os

conn = pyodbc.connect(
    "Driver=/Library/simba/spark/lib/libsparkodbc_sb64-universal.dylib;"
    f"Host={os.getenv('DATABRICKS_HOST')};"
    "Port=443;"
    f"HTTPPath={os.getenv('DATABRICKS_HTTP_PATH')};"
    "SSL=1;"
    "ThriftTransport=2;"
    "UserAgentEntry=<isv-name+product-name>/<product-version>;",
   .....
)
```

### SDKs

#### Databricks SDK for Python

With Python SDK, partners must use the `useragent.with_partner()` and `useragent.with_product()` functions to register their product and ISV identifiers for User-Agent attribution.

```python
import os
from databricks.sdk import WorkspaceClient, useragent
from databricks.sdk.core import Config

# Set partner and product identifiers
useragent.with_partner("<isv-name>")
useragent.with_product("<product-name>", "<product-version>")

# Auth from env
cfg = Config(
    host=os.getenv("DATABRICKS_HOST"),
    client_id=os.getenv("DATABRICKS_CLIENT_ID"),
    client_secret=os.getenv("DATABRICKS_CLIENT_SECRET"),
    auth_type="oauth-m2m",
)

# Workspace client
w = WorkspaceClient(config=cfg)

# Quick test
for s in w.schemas.list(catalog_name="samples"):
    print("-", s.name)
```

**Notes:**
- `with_partner()` identifies the ISV or integration owner.
- `with_product()` specifies the product name and version (version must follow SemVer).
- The User-Agent header includes these values for all SDK API requests.

#### Databricks SDK for Go

With the Go SDK, partners must use the `WithPartner()` and `WithProduct()` functions to register their product and ISV identifiers for User-Agent attribution.

```go
package main

import (
    "github.com/databricks/databricks-sdk-go"
    "github.com/databricks/databricks-sdk-go/useragent"
)

func main() {
    useragent.WithPartner("<isv-name>")
    useragent.WithProduct("<product-name>", "<product-version>")

    w := databricks.NewWorkspaceClient()
    // Example API call...
}
```

**Notes:**
- `WithPartner()` identifies the ISV or integration owner.
- `WithProduct()` sets the product name and version (version must be a valid Semantic Versioning (SemVer) version).
- Both automatically append metadata to the User-Agent header for all SDK API requests.

#### Databricks SDK for Java

With the Java SDK, partners must use the `UserAgent.withProduct()` and `UserAgent.withPartner()` methods to register their product and ISV identifiers for User-Agent attribution. The partner and product identifiers are applied globally to all SDK requests.

```java
package com.example;

import com.databricks.sdk.core.DatabricksConfig;
import com.databricks.sdk.WorkspaceClient;
import com.databricks.sdk.service.catalog.CatalogInfo;
import com.databricks.sdk.service.catalog.ListCatalogsRequest;
import com.databricks.sdk.core.UserAgent;

public class LCatalogsJSDK {
  public static void main(String[] args) {
    // Load configuration from environment
    String host = System.getenv("DATABRICKS_HOST");
    String clientId = System.getenv("DATABRICKS_CLIENT_ID");
    String clientSecret = System.getenv("DATABRICKS_CLIENT_SECRET");

    // Set partner and product identifiers
    UserAgent.withProduct("<product-name>", "<product-version>");
    UserAgent.withPartner("<isv-name>");

    // Build configuration
    DatabricksConfig config = new DatabricksConfig()
        .setHost(host)
        .setClientId(clientId)
        .setClientSecret(clientSecret);

    // Create workspace client
    WorkspaceClient client = new WorkspaceClient(config);

    // Example: List catalogs starting with 's'
    for (CatalogInfo catalog : client.catalogs().list(new ListCatalogsRequest())) {
      String name = catalog.getName();
      if (name != null && name.toLowerCase().startsWith("s")) {
        System.out.println("Catalog: " + name);
      }
    }
  }
}
```

### REST APIs

#### Unity Catalog (UC) REST APIs

With REST APIs, partners must use the User-Agent header to identify their product and ISV in every API request, following the format:

```
-H "User-Agent: <isv-name+product-name>/<product-version>"
```

**Examples:**

##### SQL Execution API

```bash
curl -X POST "$DATABRICKS_HOST/api/2.0/sql/statements" \
  -H "Authorization: Bearer $DATABRICKS_TOKEN" \
  -H "User-Agent: <isv-name+product-name>/<product-version>" \
  -H "Content-Type: application/json" \
  -d "{
    \"warehouse_id\": \"$DATABRICKS_SQL_WAREHOUSE_ID\",
    \"catalog\": \"<catalog>\",
    \"schema\": \"<schema>\",
    \"statement\": \"<your SQL here>\"
  }"
```

##### Jobs API

```bash
curl -X GET "$DATABRICKS_HOST/api/2.0/jobs/list" \
  -H "Authorization: Bearer $DATABRICKS_TOKEN" \
  -H "User-Agent: <isv-name+product-name>/<product-version>" \
  -H "Content-Type: application/json"
```

#### Iceberg Integration

With the Iceberg SDK and REST API calls, partners must include a User-Agent identifier.

**Example – Iceberg SDK (Java):**

```java
Map<String, String> props = new HashMap<>();
props.put("type", "rest");
props.put("uri", catalogApi);
props.put("warehouse", warehouse);
props.put("security", "oauth2");
props.put("oauth2-server-uri", tokenUri);
props.put("scope", "all-apis");
props.put("credential", clientId + ":" + clientSecret);

// Set User-Agent for Iceberg REST catalog
props.put("header.User-Agent", "<isv-name+product-name>/<product-version>");
System.out.println("Using User-Agent: " + props.get("header.User-Agent"));

RESTCatalog catalog = new RESTCatalog();
catalog.initialize("partner_rest", props);
```

**Example – Iceberg REST API calls:**

```java
String v1CredsUrl = catalogApi + "/v1/catalogs/" + warehouse +
              "/namespaces/" + schema + "/tables/" + table + "/credentials";

HttpURLConnection conn = (HttpURLConnection) new URL(v1CredsUrl).openConnection();
conn.setRequestMethod("GET");
conn.setRequestProperty("Authorization", "Bearer " + accessToken);
conn.setRequestProperty("User-Agent", "<isv-name+product-name>/<product-version>");
```

### Databricks Connect

With Databricks Connect for Python and Scala, partners must set the userAgent as part of the connection parameters using the session builder, to register their product and ISV identifiers for User-Agent attribution.

**Example – Scala Snippet:**

```scala
import com.databricks.connect.DatabricksSession
import org.apache.spark.sql.SparkSession

object ScalaDbConnectTest {
  def main(args: Array[String]): Unit = {

    val spark: SparkSession =
      DatabricksSession.builder
        .userAgent("<isv-name+product-name>/<product-version>")
        .getOrCreate()

    println(s"Spark Version: ${spark.version}")

    spark.sql("SELECT current_user(), current_timestamp()").show(false)

    spark.stop()
  }
}
```

**Example – Python Snippet:**

```python
from databricks.connect import DatabricksSession
from databricks.sdk.core import Config


def main():

    # --- Build SDK config from environment ---
    config = Config()

    user_agent = f"<isv-name+product-name>/<product-version>"
    print("User-Agent:", user_agent)

    # --- Create Spark session with Databricks Connect ---
    spark = (
        DatabricksSession.builder
            .sdkConfig(config)
            .userAgent(user_agent)
            .getOrCreate()
    )

    print(f"[UA: {user_agent}] Spark version: {spark.version}")

    # Verify current user/timestamp
    verify_df = spark.sql("SELECT current_user() AS user, current_timestamp() AS DateTS")
    print("[Current User/TS]:")
    verify_df.show(truncate=False)

    # --- DataFrame test ---
    print(f"[UA: {user_agent}] Running DataFrame test: spark.range(5)")
    df = spark.range(5)
    print(f"[Result]: [UA: {user_agent}]", df.collect())

    print("Done.")

if __name__ == "__main__":
    main()
```

### Libraries and UDFs

#### Python/Scala Libraries

Partners can distribute their integrations as Python or Scala, or R libraries, which can be installed on Databricks clusters using `%pip install`, library attachments, or cluster policies.

For setup and packaging details, see:
- Managing Libraries on Databricks
- Build and Install Python Wheels

#### Telemetry based on Library / Module Names

Databricks attributes library usage based on the top-level package or module name, and all sub-class names are not needed.

To ensure clear and consistent attribution, define the root package using the format:

```
<isv-name>.<product-name>
```

Databricks telemetry attributes all submodules imported from this package under the same identifier.

**Example:**

If your library's top-level package is named `isvname.datatool`, all imports such as:

```python
from isvname.datatool import client
```

Will be attributed to `isvname.datatool` in Databricks telemetry.

This ensures unambiguous tracking of ISV library usage without requiring additional configuration.

## Lakebase Integrations

### Types of Integration

These are the Lakebase integrations that support implementing the UserAgent tag.

#### Postgres JDBC Driver

The PostgreSQL JDBC driver must recognize early that the Lakebase server supports modern startup parameters so it can send the User-Agent (Application_Name) correctly during the initial connection.

Setting `ASSUME_MIN_SERVER_VERSION` ensures the driver treats the target server as a modern PostgreSQL-compatible server and includes the application_name in the initial startup packet. Without this setting, some clients delay sending the application_name setting until after the connection is established, which prevents telemetry from capturing it. By explicitly setting the connection property `ASSUME_MIN_SERVER_VERSION` (for example, to 9.1), the driver reliably includes the application name in the startup packet.

When using a connection string, the parameter name is typically `Application_Name` (in camelCase), although it may vary by driver version; therefore, partners need to verify the exact parameter name in the JDBC driver's documentation.

Partners must set it programmatically using PGProperty, as shown below. This configuration ensures telemetry and diagnostics in Databricks Lakebase capture the User-Agent information as part of the initial connection handshake.

**Example:**

```java
// Set up connection properties
Properties props = new Properties();
PGProperty.USER.set(props, "<email>");
PGProperty.PASSWORD.set(props, "<OAuth token>");
PGProperty.APPLICATION_NAME.set(props, "<isv-name+product-name>/<product-version>");
PGProperty.ASSUME_MIN_SERVER_VERSION.set(props, "9.1");
PGProperty.SSL.set(props, "require");

Connection conn = DriverManager.getConnection(jdbcUrl, props);
```

#### Other Integrations

The psql, psycopg, and SQLAlchemy parameter is `application_name` (all lowercase).

Here is an example string to be set by the ISV Partner product for Lakebase integration:

```
ISVApplicationName = <isv-name+product-name>/<product-version>
```

##### Psql Client

```bash
psql "host=instance-*** user=*** dbname=*** port=5432 sslmode=require application_name=ISVApplicationName"
```

##### psycopg2/3 (Python)

```python
conn = psycopg2.connect(
    host="instance-***",
    port=5432,
    dbname="dbname",
    user="user",
    password="<OAuth token>",
    application_name="ISVApplicationName"
)
```

##### SQLAlchemy (Python)

```python
create_engine(
    "postgresql+psycopg2://user:<OAuth token>@host:5432/dbname?application_name=ISVApplicationName"
)
```
